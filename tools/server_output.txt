Loading model from D:\dev\AI_Interview\tools\text-generation-webui\models\llama-2-7b-chat.Q4_K_M.gguf...
Model loaded successfully!
Starting Llama-2 server on http://localhost:8080
Endpoint: POST /completion
Format: {"prompt": "...", "max_tokens": 500, "temperature": 0.7}
 * Serving Flask app 'llama_server'
 * Debug mode: off
