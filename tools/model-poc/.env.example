# Backend Configuration
# Options: local, hf, openai
BACKEND=local

# Local Model Server
LOCAL_MODEL_PATH=models/llama-2-7b-chat.Q4_K_M.gguf
LOCAL_MODEL_URL=http://localhost:8080/completion
LOCAL_MODEL_TYPE=llamacpp
# LOCAL_MODEL_TYPE=webui for text-generation-webui

# Hugging Face (if BACKEND=hf)
HF_TOKEN=your_huggingface_token_here
HF_MODEL=meta-llama/Llama-2-7b-chat-hf

# OpenAI (if BACKEND=openai)
OPENAI_API_KEY=your_openai_key_here
OPENAI_MODEL=gpt-3.5-turbo

# Proxy Server
PROXY_PORT=3001
