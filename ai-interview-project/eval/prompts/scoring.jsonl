{"id":"SC-01","task_type":"scoring","difficulty":"medium","prompt":"IMPORTANT: Respond ONLY with a single valid JSON object matching the `scoring` schema: {\"score\": number (0-100), \"rationale\": string, \"breakdown\": object}. Output only the JSON, no explanation. Example: {\"score\": 82, \"rationale\": \"Answer demonstrates correct approach but lacks performance considerations.\", \"breakdown\": {\"accuracy\": 40, \"completeness\": 30, \"clarity\": 12}}\nQuestion: Explain how you would design a cache for a high-traffic API. Candidate answer: \"I would use an LRU cache in front of the DB with TTLs and async refresh.\"\nPlease score the candidate answer per the rubric and provide a short rationale.","input_context":null,"expected_behaviors":["Scores are numeric and justified","Breakdown maps to sub-criteria"],"evaluation_criteria":["consistency","explainability","usefulness"]}
{"id":"SC-02","task_type":"scoring","difficulty":"hard","prompt":"IMPORTANT: Respond ONLY with a single valid JSON object matching the `scoring` schema: {\"score\": number (0-100), \"rationale\": string, \"breakdown\": object}. Output only the JSON, no explanation. Example: {\"score\": 74, \"rationale\": \"Good structure but missing edge cases.\", \"breakdown\": {\"accuracy\": 30, \"depth\": 24, \"clarity\": 20}}\nQuestion: How would you design a zero-downtime schema migration for a table with billions of rows? Candidate answer: \"Use online schema changes with shadow tables and backfill.\"\nScore and justify.","input_context":null,"expected_behaviors":["Explicit numeric score","Short, evidence-based rationale"],"evaluation_criteria":["accuracy","depth","practicality"]}

